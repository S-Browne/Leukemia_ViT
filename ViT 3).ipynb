{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7qtZf14CD6Os"},"outputs":[],"source":["#imports\n","import numpy as np\n","from tqdm import tqdm, trange\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import ToTensor\n","import os\n","from posix import listdir\n","from random import shuffle\n","from math import floor\n","import re\n","import tensorflow as tf\n","import cv2\n","import matplotlib.pyplot as plt\n","from numpy.ma.core import argmin\n","from matplotlib.colors import LinearSegmentedColormap\n","from PIL import Image\n","import random\n","from keras.preprocessing.image import ImageDataGenerator\n","import PIL\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1691661140510,"user":{"displayName":"Stefan Browne","userId":"07394413646385420152"},"user_tz":-60},"id":"drS7jAMmEDA2","outputId":"d658048e-0397-4d77-c85a-3477cf57bd04"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x78c9143e8310>"]},"metadata":{},"execution_count":2}],"source":["#this makes the model deterministic, and thus more repeatable\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2wsgdRXbEDDo"},"outputs":[],"source":["path = '/content/drive/MyDrive/ALL_IDB2/img/'         #main directory for training data\n","dirs = listdir(path)                                  # lists items in training directory\n","n_p = 8   #number of patches, used both in patchifying and explaining: defined here because the value needs to be same for both"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3E3-84H2EJUb"},"outputs":[],"source":["'''\n","This function augments the data\n","The filename of each file is extracted\n","The image iteslf is turned into an array, and 8 augmented images are produced from it:\n","1) the image is rotated in a random dircetion by up to 40 degrees\n","2) & 3) flips are permormed about both the vertical and horizontal axes\n","4) & 5) translations are carried out in a random direction by 0-40% of the image width in either x or y directions\n","6) the image is zoomed into at 30% zoom\n","7) the image is sheared by 20%\n","8) an unmodified 'normal' image is outputted\n","'''\n","\n","def augment():\n"," for item in dirs:\n","        if os.path.isfile(path+item):\n","            f, e = os.path.splitext(path+item)\n","            img1 = cv2.imread(path+item)\n","            img2 = cv2.resize(img1, (512,512), interpolation = cv2.INTER_AREA)\n","            img3 = tf.keras.utils.img_to_array(img2)\n","\n","            #rotation\n","            r1 = np.expand_dims(img3, 0)\n","            r2 = ImageDataGenerator(rotation_range=40).flow(r1).next()\n","            r3 = r2[0].astype('uint8')\n","            cv2.imwrite(f + 'r40n.jpg', r3)\n","\n","            #flip, vertical\n","            flippedv = cv2.flip(img3, 1)\n","            cv2.imwrite(f + 'fvn.jpg', flippedv)\n","\n","            #flip, horizontal\n","            flippedh = cv2.flip(img3, 0)\n","            cv2.imwrite(f + 'fhn.jpg', flippedh)\n","\n","            #x-translate\n","            tx1 = np.expand_dims(img3, 0)\n","            tx2 = ImageDataGenerator(height_shift_range=0.4).flow(tx1).next()\n","            tx3 = tx2[0].astype('uint8')\n","            cv2.imwrite(f + 'tx40n.jpg', tx3)\n","\n","            #y-translate\n","            ty1 = np.expand_dims(img3, 0)\n","            ty2 = ImageDataGenerator(width_shift_range=0.4).flow(ty1).next()\n","            ty3 = ty2[0].astype('uint8')\n","            cv2.imwrite(f + 'ty40n.jpg', ty3)\n","\n","            #zoom\n","            z1 = np.expand_dims(img3, 0)\n","            z2 = ImageDataGenerator(zoom_range=[1/1.3,1/1.3]).flow(z1).next()\n","            z3 = z2[0].astype('uint8')\n","            cv2.imwrite(f + 'z30n.jpg', z3)\n","\n","            #shear\n","            s1 = np.expand_dims(img3, 0)\n","            s2 = ImageDataGenerator(shear_range=20).flow(s1).next()\n","            s3 = s2[0].astype('uint8')\n","            cv2.imwrite(f + 's20n.jpg', s3)\n","\n","            #normal image\n","            cv2.imwrite(f + 'n.jpg', img3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e98f5mctEDGj"},"outputs":[],"source":["'''\n","This function generates five datasets from the images files in the training folder\n","It starts by creating a list of all the files of the correct dimensions (with correct extensions)\n","The label for each object is then found using regular expressions\n","the label is added to the name in a 2-element list\n","the list is shuffled and split into four parts\n","Four different training/testing sets are created by removing one of the four parts at a time for use as a validation set\n","For each of the two datasets in turn, the filenames are converted into arrays and replaced by these arrays\n","'''\n","def data_fold():\n","\n"," folds = 4\n"," global datasets_test                        #initiate data arrays\n"," datasets_test = [[0]*(27*8)]*folds\n"," global datasets_train\n"," datasets_train = [[0]*(81*8)]*folds\n","\n"," i = 0                                      # initiate data-writing loop\n"," while i < folds:\n","\n","  all_files = os.listdir(os.path.abspath(path))    #get list of files to process\n","  data_files = list(filter(lambda file: file.endswith('n.jpg'), all_files))\n","  shuffle(data_files)\n","\n","  data_files = np.array(data_files)                         # split into testing and training data\n","  split_set = np.split(data_files,folds)\n","  global train_set\n","  train_set = list(np.concatenate((split_set[((i+0)%folds)], split_set[((i+1)%folds)], split_set[((i+2)%folds)])))\n","  test_set = list(split_set[((i+3)%folds)])\n","\n","  j = 0\n","  p = re.compile(r'_1')\n","  while j<len(train_set):\n","   c = cv2.imread(path+str(train_set[j]))\n","   d = tf.keras.utils.img_to_array(c)\n","   if (p.findall(train_set[j]) == [str('_1')]):\n","    h = (1)\n","   elif (p.findall(train_set[j]) != [str('_1')]):\n","    h = (0)\n","   datasets_train[i][j] = [d,h]\n","   j+=1\n","\n","  j = 0\n","  p = re.compile(r'_1')\n","  while j<len(test_set):\n","   c = cv2.imread(path+str(test_set[j]))\n","   d = tf.keras.utils.img_to_array(c)\n","   if (p.findall(test_set[j]) == [str('_1')]):\n","    h = (1)\n","   elif (p.findall(test_set[j]) != [str('_1')]):\n","    h = (0)\n","   datasets_test[i][j] = [d,h]\n","   j+=1\n","\n","  i+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkR-b6AFTEHa"},"outputs":[],"source":["def main():\n","    # Loading data\n","    train_loader = DataLoader(train, shuffle=True)     #loading data sets\n","    test_loader = DataLoader(test, shuffle=False)\n","\n","    # Defining model and training options\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    #choosing processing device\n","    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")  # printing name of device being used\n","    global model\n","    model = MyViT((3, 512, 512), n_patches=8, n_blocks=4, hidden_d=768, n_heads=2, out_d=2).to(device)     # params\n","    N_EPOCHS = 15\n","    LR = 0.0000002\n","\n","    # Training loop\n","    optimizer = Adam(model.parameters(), lr=LR)    #optimizer, lr = learning rate\n","    #optimizer = torch.optim.SGD(model.parameters(), lr=LR)       # use this line instead of above line if you want SGD insetad of ADAM\n","    criterion = CrossEntropyLoss()                                  #loss criterion\n","    for epoch in trange(N_EPOCHS, desc=\"Training\"):                 #loop for training epoch\n","        train_loss = 0.0\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):        #loop for training within epoch\n","            x, y = batch              #get data, and its label\n","            y = y.to(device)          #send data to GPU/CPU\n","            x = x.to(device)\n","            y_hat = model(x)                           #predicted value of data\n","            loss = criterion(y_hat, y)                 #loss due to predicted value and exapt value being different\n","\n","            train_loss += loss.detach().cpu().item() / len(train_loader)  #calculating total loss\n","\n","            optimizer.zero_grad()                 #optimization of params\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.4f}\")    #reporting training accuracy (loss)\n","\n","    # Test loop\n","    with torch.no_grad():\n","        correct, total = 0, 0\n","        test_loss = 0.0\n","        for batch in tqdm(test_loader, desc=\"Testing\"):     # training loop over all data\n","            x, y = batch                                    # extract data and label\n","            x, y = x.to(device), y.to(device)               # sending data to be processed\n","            y_hat = model(x)                                # prediction of model\n","            loss = criterion(y_hat, y)\n","            test_loss += loss.detach().cpu().item() / len(test_loader)\n","\n","            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()      #increses number of correct if value of y is same as column in output matrix with max probability\n","            total += len(x)\n","\n","        print(f\"Test loss: {test_loss:.2f}\")      #reporting results\n","        print(f\"Test accuracy: {correct / total * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FnPb9o_TXR9"},"outputs":[],"source":["'''\n","This cell contains all of the code for the functions and objects which the ViT model uses, and defines them\n","'''\n","\n","'''\n","Patchify cuts the image into squares for feeding into the transformer\n","First it makes sure that the image is square\n","Then it creates a tensor to store the patches\n","Finally it selects squares from the image, flattens them, and adds them to the tensor it has created\n","'''\n","def patchify(images, n_patches):\n","    n, h, w, c = images.shape\n","\n","    assert h == w, \"Patchify method is implemented for square images only\"    #making sure image is square\n","\n","    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)      #creates a tensor to store patches\n","    patch_size = h // n_patches\n","\n","    for idx, image in enumerate(images):                  #patchify image\n","        for i in range(n_patches):                      #horizontal and vertical cuts\n","            for j in range(n_patches):\n","                patch = image[i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size,:]   #patchifies by taking pixels in relavent range of h,w\n","                patches[idx, i * n_patches + j] = patch.flatten()            #turns patch into vector\n","    return patches\n","\n","\n","\n","'''\n","This function adds positional encodings to a new tensor\n","This means that the model can now know where each patch in original image came from\n","This is because the value is uniquely dependant on the position of each patch\n","'''\n","def get_positional_embeddings(sequence_length, d):\n","    result = torch.ones(sequence_length, d)          #creates a tensor with dimensions ,sequence_length, d\n","    for i in range(sequence_length):\n","        for j in range(d):\n","            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))   #loop writes positional encodings into tensor\n","    return result\n","\n","'''\n","This class defines the operation of a self-attention head\n","'''\n","class MyMSA(nn.Module):\n","    def __init__(self, d, n_heads=4):\n","        super(MyMSA, self).__init__()\n","        self.d = d\n","        self.n_heads = n_heads\n","\n","        #Dimensions of vector must be divisible between heads so that all heads process equivalent ammounts of data for each image\n","        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n","\n","        d_head = int(d / n_heads)          #finding the dimension of the input of a head\n","        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)]) #query vector\n","        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)]) #key vector\n","        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)]) #learned vector\n","        self.d_head = d_head\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, sequences):\n","        # Sequences has shape (N, seq_length, token_dim)\n","        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n","        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n","        result = []\n","        for sequence in sequences:\n","            seq_result = []\n","            for head in range(self.n_heads):\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","\n","                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n","                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n","\n","                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))      #application of self-attention\n","                seq_result.append(attention @ v)\n","            result.append(torch.hstack(seq_result))\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])        #combine the outputs of the different heads\n","\n","'''\n","This class defines a transformer block\n","'''\n","class MyViTBlock(nn.Module):\n","    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n","        super(MyViTBlock, self).__init__()\n","        self.hidden_d = hidden_d\n","        self.n_heads = n_heads\n","        #These are defenitions of various attributes and metods of the block\n","        self.norm1 = nn.LayerNorm(hidden_d)\n","        self.mhsa = MyMSA(hidden_d, n_heads)\n","        self.norm2 = nn.LayerNorm(hidden_d)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n","            nn.GELU(),\n","            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n","        )\n","    #THis function is what produces the output of the transformer's block, and feeds it forwrd\n","    def forward(self, x):\n","        out = x + self.mhsa(self.norm1(x))\n","        out = out + self.mlp(self.norm2(out))\n","        return out\n","\n","'''\n","This class defines the overall operation of the whole model\n","The various functions which it uses are described where they are defined\n","'''\n","class MyViT(nn.Module):\n","    def __init__(self, hwc, n_patches=n_p, n_blocks=4, hidden_d=256, n_heads=4, out_d=2):\n","        # Super constructor\n","        super(MyViT, self).__init__()\n","\n","        # Attributes\n","        self.hwc = hwc # (Height , Width , Colour)\n","        self.n_patches = n_patches\n","        self.n_blocks = n_blocks\n","        self.n_heads = n_heads\n","        self.hidden_d = hidden_d\n","\n","        # Input and patches sizes\n","        assert hwc[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","        assert hwc[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","        self.patch_size = (hwc[1] / n_patches, hwc[2] / n_patches)\n","\n","        # 1) Linear mapper for turning matrices into smaller vectors\n","        self.input_d = int(hwc[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n","\n","        # 2) Learnable classification token\n","        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n","\n","        # 3) Positional embedding\n","        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n","\n","        # 4) Transformer encoder blocks (makes them part of the model)\n","        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n","\n","        # 5) Classification MLPk\n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.hidden_d, out_d),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        # Dividing images into patches\n","        n, h, w, c = images.shape   #extracts shapes from image data\n","        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device) #creates patches\n","\n","        # Running linear layer tokenization\n","        # Map the vector corresponding to each patch to the hidden size dimension (hidden_d)\n","        tokens = self.linear_mapper(patches)\n","\n","        # Adding classification token to the tokens (patches)\n","        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n","\n","        # Adding positional embedding so that model know where patch came from\n","        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n","\n","        # Transformer Blocks\n","        #The output of each block is calculated\n","        for block in self.blocks:\n","            out = block(out)\n","\n","        # Getting the classification token only\n","        out = out[:, 0]\n","\n","        return self.mlp(out) # Map to output dimension, output category distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-WGt0syTaJE"},"outputs":[],"source":["'''\n","This trains the model\n","It augments the data set\n","It then creates a four-fold validation set\n","Lastly it trains and validates the model on each of the folds\n","'''\n","def prepare():\n","  #augment()\n","  data_fold()\n","\n","  k = 0\n","  while k<4:\n","    global train\n","    train = datasets_train[k]\n","    global test\n","    test = datasets_test[k]\n","    main()\n","    k+=5"]},{"cell_type":"code","source":["prepare()"],"metadata":{"id":"dfAdIIZmBKbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVrIHksRuPXZ"},"outputs":[],"source":["def use():\n","    # Loading data\n","    use_path = '/content/drive/MyDrive/ALL_IDB1/im/'\n","    all_files = os.listdir(os.path.abspath(use_path))    #get list of files to process\n","    data_files = list(filter(lambda file: file.endswith('.jpg'), all_files))\n","    size_use = int(len(data_files))\n","\n","    datasets_use = [0]*size_use\n","    j = 0\n","    while j < size_use:\n","     a = str(data_files[j])\n","     b = cv2.imread(use_path+str(data_files[j]))\n","     c = cv2.resize(b, (512,512), interpolation = cv2.INTER_CUBIC)\n","     datasets_use[j] = [tf.keras.utils.img_to_array(c), a]\n","     j+=1\n","\n","    test_loader = DataLoader(datasets_use)\n","\n","    # Use loop\n","    Correct = 0\n","    FP = 0\n","    FN = 0\n","    p = re.compile(r'_1')\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            x,y_1 = batch\n","            v = model(x)\n","            pred = int(torch.argmax(v))\n","            print(y_1, pred)\n","\n","            if (p.findall(str(y_1)) == [str('_1')]):\n","               h = 1\n","            elif (p.findall(str(y_1)) != [str('_1')]):\n","              h = 0\n","            if h == pred:\n","              Correct+=1\n","            elif h == 1 and pred == 0:\n","              FN += 1\n","            elif h == 0 and pred == 1:\n","              FP += 1\n","\n","    print(f'correct = {Correct}')\n","    print(f'False Negative = {FN}')\n","    print(f'False Positive = {FP}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkzAxu6ZNmaK"},"outputs":[],"source":["use()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFwwRRHPXkLG"},"outputs":[],"source":["'''\n","This function tries to explain the model's output using oclusion.\n","This is a peturbation based aproach where square patches of the image are replaced by zeroes\n","and the difference between the output for this image and the old image is computed and plotted\n","The 'explain_path' is the source for the data, and all the files in this folder are listed\n","A data set where each image is represented as a tensor and labelled by its name is then generated\n","For each element in the data set, the model is used to predict whether cancer is present\n","Occulsion is implemented and patches of the image are zeroed; the whole image is covered\n","The diiferences are put into an array with the same dimensions as the input image\n","The array is normalised so that its values are between 0. and 1.\n","The array is plotted as a cmap\n","'''\n","def explain():\n","    #get list of files\n","    explain_path = '/content/drive/MyDrive/ALL_IDB1/im/'\n","    all_files = os.listdir(os.path.abspath(explain_path))    #get list of files to process\n","\n","    data_files = list(filter(lambda file: file.endswith('n.jpg'), all_files))\n","    size_explain = int(len(data_files))\n","\n","    #get dataset\n","    datasets_use = [0]*size_explain\n","    j = 0\n","    while j < size_explain:\n","     a = str(data_files[j])\n","     b = cv2.imread(explain_path+str(data_files[j]))\n","     c = cv2.resize(b, (512,512), interpolation = cv2.INTER_AREA)\n","     datasets_use[j] = [tf.keras.utils.img_to_array(c), a]\n","     j+=1\n","\n","    explain_loader = DataLoader(datasets_use, shuffle = True)\n","\n","\n","    with torch.no_grad():\n","        for batch in explain_loader:\n","            x,y_1 = batch                          #get model prediction\n","            v = model(x)                           #produces array where elemnt i is the probability that image is in ith class\n","            pred = int(torch.argmax(v))\n","            print(y_1[0],';', v)\n","\n","\n","            i = 0\n","            global attr\n","            attr = np.array([[0]*256]*256)\n","            while i < n_p:                      #horizontal and vertical cuts\n","             j = 0\n","             while j < n_p:\n","              q = np.array(x)\n","\n","              for k in range(int(256/n_p)):                                #turning the relevant patch into a set of zeroes\n","                for l in range(int(256/n_p)):\n","                 q[0][int(i*256/n_p) + k][int(j*256/n_p) + l][:] = 0\n","\n","              r = torch.from_numpy(q)\n","              s = model(r)\n","              diff = float(v[0][1])-float(s[0][1])                      #calculating the resulting change in the model\n","\n","              for k in range(int(256/n_p)):\n","                for l in range(int(256/n_p)):\n","                 attr[int(i*256/n_p) + k][int(j*256/n_p) + l] = diff*10**15      #putting these attributions of regions into an array\n","              j+=1\n","             i+=1\n","\n","            Max = float(np.max(np.array(attr)))\n","            Min = float(np.min(np.array(attr)))\n","            attr_1 = (np.array(attr) - Min)/(Max - Min)                   #normalising the array\n","\n","            plt.imshow(attr_1, cmap='Greys')\n","            plt.colorbar()\n","            plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ido0XIvYX0HS","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1aAvk9YOPeqR3_99OOWdHK5W0fheVVcuD"},"outputId":"daed3ebf-ff04-4c0f-f35d-7fd94406d267"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["explain()"]}],"metadata":{"colab":{"provenance":[{"file_id":"11JKrMD6b7_EPoxgjv6DCvUPXl6BEQkyd","timestamp":1690357736853}],"mount_file_id":"1tQsjuZyXUXrwm7VnIfAxllRuGm0SYGxY","authorship_tag":"ABX9TyMBw4W0RqzFKUvt/1g8jjJf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}